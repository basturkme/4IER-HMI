import os
import scipy.io
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# --- 1. AYARLAR ---
dosya_yolu = '/content/drive/MyDrive/PokeD_ML/NINAPRODB2/S1_E1_A1.mat'
MODEL_ADI = 'robust_emg_model'

# Veriyi Yükle
if not os.path.exists(dosya_yolu):
    raise SystemExit("Dosya bulunamadı!")

mat = scipy.io.loadmat(dosya_yolu)
df = pd.DataFrame(mat['emg'])
df['Label'] = mat['restimulus']

# Hedef: İşaret Parmağı (1)
df_hareket = df[df['Label'] == 1].copy()
df_dinlenme = df[df['Label'] == 0].copy()

# Eşitleme (Undersampling)
if len(df_hareket) > 0:
    df_dinlenme_esit = df_dinlenme.sample(n=len(df_hareket), random_state=42)
    df_filtered = pd.concat([df_hareket, df_dinlenme_esit])
    df_filtered['Label'] = df_filtered['Label'].apply(lambda x: 1 if x == 1 else 0)
    df_filtered = df_filtered.sample(frac=1, random_state=42).reset_index(drop=True)
else:
    raise SystemExit("Hedef hareket bulunamadı")

X = df_filtered[[0, 1, 2, 3]].values
y = df_filtered['Label'].values

# --- TANI KODU: VERİ KONTROLÜ VE DÜZELTME ---
print("1. Sınıf Dağılımı:")
print(df_filtered['Label'].value_counts()) 

print("\n--- KRİTİK DÜZELTME ADIMI ---")
raw_max = np.max(np.abs(X))
print(f"Ham Sinyal Maksimum Değeri (Eski): {raw_max:.6f}")

# 1. ADIM: SİNYALİ GÜÇLENDİR (0-1 Arasına Çek)
# Tüm sinyali mutlak maksimum değerine bölüyoruz.
# Böylece en yüksek sinyal 1.0 oluyor.
global_max = np.max(np.abs(X))
if global_max > 0:
    X = X / global_max
    print(f"Sinyal Normalize Edildi. Yeni Maksimum: {np.max(np.abs(X)):.2f}")
    # Bu katsayıyı Arduino tarafında da kullanmak gerekebilir, not edelim.
    print(f"NOT: Arduino'da gelen sinyali bu sayı ile çarpmak gerekebilir (veya StandardScaler halleder): {1/global_max:.2f}")

# 2. ADIM: GÜRÜLTÜ EKLE
# Artık sinyalimiz 1.0 seviyesinde olduğu için 0.05 (Yüzde 5) gürültü makuldür.
noise_std = 0.0005
print(f"Eklenen Gürültü Standart Sapması: {noise_std}")

# Gürültü Ekleme (Sağlamlık için)
noise = np.random.normal(0, noise_std, X.shape)
X_noisy = X + noise

# 3. ADIM: STANDARTLAŞTIRMA (Scale)
# Neural Network'ler için veriyi merkeze (0) çeker.
scaler = StandardScaler()
X_train = scaler.fit_transform(X_noisy) # Gürültülü ve güçlendirilmiş veriyi eğit

print("\n3. Veri Örneği (İşlenmiş):")
print(X_train[0])

# --- 2. GÜÇLÜ DENSE MODEL ---
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(4,)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.1), 
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Eğitimi Başlat
print("\n--- EĞİTİM BAŞLIYOR ---")
history = model.fit(X_train, y, epochs=30, batch_size=16, verbose=1)

# --- 3. DOSYALARI OLUŞTUR ---
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

def hex_to_c_array(data, var_name):
    c_str = ''
    for i, val in enumerate(data):
        c_str += f'0x{val:02x}, '
        if (i + 1) % 12 == 0: c_str += '\n  '
    return f'const unsigned char {var_name}[] = {{\n  {c_str[:-2]}\n}};\nconst unsigned int {var_name}_len = {len(data)};'

with open(f'{MODEL_ADI}_data.h', 'w') as f:
    f.write(hex_to_c_array(tflite_model, 'emg_model_data'))

# Test Vektörü Hazırlığı
# DİKKAT: Test vektörleri de aynı işlemden geçmeli (Önce X/global_max, sonra scaler)
X_clean = df[[0, 1, 2, 3]].values
X_clean_norm = X_clean / global_max # Aynı katsayı ile bölüyoruz
X_clean_scaled = scaler.transform(X_clean_norm) # Aynı scaler ile dönüştürüyoruz

# Dinlenme ve Hareket indeksleri
idx_rest = np.where(df['Label'] == 0)[0]
idx_move = np.where(df['Label'] == 1)[0]

# Sıralı Senaryo
test_seq = np.concatenate((
    X_clean_scaled[idx_rest[0:50]],
    X_clean_scaled[idx_move[100:150]],
    X_clean_scaled[idx_rest[200:250]],
    X_clean_scaled[idx_move[300:350]]
))

def generate_test_header(data):
    c_str = f'const int TEST_DATA_LEN = {len(data)};\nconst float test_data[][4] = {{\n'
    for row in data:
        c_str += '    {' + ', '.join([f'{x:.4f}' for x in row]) + '},\n'
    c_str += '};\n'
    return c_str

with open('test_vectors.h', 'w') as f:
    f.write("#ifndef TEST_VECTORS_H\n#define TEST_VECTORS_H\n\n")
    f.write(generate_test_header(test_seq))
    f.write("\n#endif")

# --- GRAFİK KISMI ---
FILTER_ALPHA = 0.2
THRESHOLD = 0.4

test_predictions = model.predict(test_seq)
raw_probs = test_predictions[:, 1]

filtered_probs = []
current_val = 0.0

for prob in raw_probs:
    current_val = (FILTER_ALPHA * prob) + ((1.0 - FILTER_ALPHA) * current_val)
    filtered_probs.append(current_val)

filtered_probs = np.array(filtered_probs)

plt.figure(figsize=(15, 6))
plt.plot(raw_probs, label='Ham Model Çıktısı (Titrek)', color='lightgray', linestyle='--')
plt.plot(filtered_probs, label=f'Filtrelenmiş Çıktı (Alpha={FILTER_ALPHA})', color='red', linewidth=2)
plt.axhline(y=THRESHOLD, color='green', linestyle=':', label='Hareket Eşiği')
plt.title('EMG Model Çıktısı (DÜZELTİLMİŞ EĞİTİM)')
plt.ylabel('Hareket Olasılığı (0-1)')
plt.xlabel('Zaman (Örnek Sayısı)')
plt.legend()
plt.grid(True, alpha=0.3)

fill_regions = filtered_probs > THRESHOLD
plt.fill_between(range(len(filtered_probs)), 0, 1, where=fill_regions, color='green', alpha=0.1, transform=plt.gca().get_xaxis_transform())

plt.show()

print("İndirilecek dosyalar hazırlanıyor...")
from google.colab import files
try:
    files.download(f'{MODEL_ADI}_data.h')
    files.download('test_vectors.h')
except:
    print("Dosya indirme işlemi Colab arayüzünden manuel yapılabilir.")
